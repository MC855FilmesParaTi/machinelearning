{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pipeline do Modelo Final\n",
    "\n",
    "Com este notebook, basta substituir os datasets por outros processados e rodar as células, e assim, obter previsões."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 30426,
     "status": "ok",
     "timestamp": 1669052473458,
     "user": {
      "displayName": "Marcela Medicina",
      "userId": "17288344570535601151"
     },
     "user_tz": 180
    },
    "id": "-uC_ohs2-dGc",
    "outputId": "be6de27d-fdae-4793-8572-6fbb98ea62e7"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 5179,
     "status": "ok",
     "timestamp": 1669052490522,
     "user": {
      "displayName": "Marcela Medicina",
      "userId": "17288344570535601151"
     },
     "user_tz": 180
    },
    "id": "Np_J2YMW-9ys"
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import os\n",
    "import shutil\n",
    "# import papermill as pm\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from reco_utils.recommender.ncf.dataset import Dataset as NCFDataset\n",
    "from reco_utils.dataset import movielens\n",
    "from reco_utils.dataset.python_splitters import python_chrono_split\n",
    "from reco_utils.evaluation.python_evaluation import (rmse, mae, rsquared, exp_var, map_at_k, ndcg_at_k, precision_at_k, \n",
    "                                                     recall_at_k, get_top_k_items)\n",
    "from reco_utils.common.constants import SEED as DEFAULT_SEED"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 311,
     "status": "ok",
     "timestamp": 1669052719447,
     "user": {
      "displayName": "Marcela Medicina",
      "userId": "17288344570535601151"
     },
     "user_tz": 180
    },
    "id": "Zo4Q2VBwEsnS"
   },
   "outputs": [],
   "source": [
    "# Copyright (c) Microsoft Corporation. All rights reserved.\n",
    "# Licensed under the MIT License.\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import tensorflow.compat.v1 as v1\n",
    "tf.compat.v1.disable_eager_execution()\n",
    "from time import time\n",
    "import logging\n",
    "\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "\n",
    "class NCF:\n",
    "    \"\"\"NCF implementation\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        n_users,\n",
    "        n_items,\n",
    "        model_type=\"NeuMF\",\n",
    "        random_state=0,\n",
    "        n_factors=8,\n",
    "        layer_sizes=[16, 8, 4],\n",
    "        n_epochs=50,\n",
    "        batch_size=64,\n",
    "        learning_rate=5e-3,\n",
    "        verbose=1,\n",
    "        save=False,\n",
    "        pretrain=False,\n",
    "        seed=42,\n",
    "    ):\n",
    "        # number of users in dataset\n",
    "        self.n_users = n_users\n",
    "        # number of items in dataset\n",
    "        self.n_items = n_items\n",
    "        # model type\n",
    "        self.model_type = model_type.lower()\n",
    "        # check model type\n",
    "        model_options = [\"gmf\", \"mlp\", \"neumf\"]\n",
    "        if self.model_type not in model_options:\n",
    "            raise ValueError(\n",
    "                \"Wrong model type, please select one of this list: {}\".format(\n",
    "                    model_options\n",
    "                )\n",
    "            )\n",
    "        # seed\n",
    "        tf.random.set_seed(seed)\n",
    "        np.random.seed(seed)\n",
    "        # dimension of latent space\n",
    "        self.n_factors = n_factors\n",
    "        # number of layers for mlp\n",
    "        self.layer_sizes = layer_sizes\n",
    "        # number of epochs for training\n",
    "        self.n_epochs = n_epochs\n",
    "        # training output or not\n",
    "        self.verbose = verbose\n",
    "        # set batch size\n",
    "        self.batch_size = batch_size\n",
    "        # set learning rate\n",
    "        self.learning_rate = learning_rate\n",
    "        # ncf layer input size\n",
    "        self.ncf_layer_size = n_factors + layer_sizes[-1]\n",
    "        # create ncf model\n",
    "        self._create_model()\n",
    "        # set GPU use with demand growth\n",
    "        gpu_options = v1.GPUOptions(allow_growth=True)\n",
    "        # set TF Session\n",
    "        self.sess = v1.Session(config=v1.ConfigProto(gpu_options=gpu_options))\n",
    "        # parameters initialization\n",
    "        self.sess.run(v1.global_variables_initializer())\n",
    "\n",
    "    def _create_model(self,):\n",
    "        # reset graph\n",
    "        from tensorflow.python.framework import ops\n",
    "        ops.reset_default_graph()\n",
    "\n",
    "        with tf.compat.v1.variable_scope(\"input_data\", reuse=tf.compat.v1.AUTO_REUSE):\n",
    "\n",
    "            # input: index of users, items and ground truth\n",
    "            self.user_input = v1.placeholder(tf.int32, shape=[None, 1])\n",
    "            self.item_input = v1.placeholder(tf.int32, shape=[None, 1])\n",
    "            self.labels = v1.placeholder(tf.float32, shape=[None, 1])\n",
    "\n",
    "        with v1.variable_scope(\"embedding\", reuse=tf.compat.v1.AUTO_REUSE):\n",
    "\n",
    "            # set embedding table\n",
    "            self.embedding_gmf_P = tf.Variable(\n",
    "                v1.truncated_normal(\n",
    "                    shape=[self.n_users, self.n_factors], mean=0.0, stddev=0.01\n",
    "                ),\n",
    "                name=\"embedding_gmf_P\",\n",
    "                dtype=tf.float32,\n",
    "            )\n",
    "\n",
    "            self.embedding_gmf_Q = tf.Variable(\n",
    "                v1.truncated_normal(\n",
    "                    shape=[self.n_items, self.n_factors], mean=0.0, stddev=0.01\n",
    "                ),\n",
    "                name=\"embedding_gmf_Q\",\n",
    "                dtype=tf.float32,\n",
    "            )\n",
    "\n",
    "            # set embedding table\n",
    "            self.embedding_mlp_P = tf.Variable(\n",
    "                v1.truncated_normal(\n",
    "                    shape=[self.n_users, int(self.layer_sizes[0] / 2)],\n",
    "                    mean=0.0,\n",
    "                    stddev=0.01,\n",
    "                ),\n",
    "                name=\"embedding_mlp_P\",\n",
    "                dtype=tf.float32,\n",
    "            )\n",
    "\n",
    "            self.embedding_mlp_Q = tf.Variable(\n",
    "                v1.truncated_normal(\n",
    "                    shape=[self.n_items, int(self.layer_sizes[0] / 2)],\n",
    "                    mean=0.0,\n",
    "                    stddev=0.01,\n",
    "                ),\n",
    "                name=\"embedding_mlp_Q\",\n",
    "                dtype=tf.float32,\n",
    "            )\n",
    "\n",
    "        with tf.compat.v1.variable_scope(\"gmf\", reuse=tf.compat.v1.AUTO_REUSE):\n",
    "\n",
    "            # get user embedding p and item embedding q\n",
    "            self.gmf_p = tf.reduce_sum(\n",
    "                tf.nn.embedding_lookup(self.embedding_gmf_P, self.user_input), 1\n",
    "            )\n",
    "            self.gmf_q = tf.reduce_sum(\n",
    "                tf.nn.embedding_lookup(self.embedding_gmf_Q, self.item_input), 1\n",
    "            )\n",
    "\n",
    "            # get gmf vector\n",
    "            self.gmf_vector = self.gmf_p * self.gmf_q\n",
    "\n",
    "        with tf.compat.v1.variable_scope(\"mlp\", reuse=tf.compat.v1.AUTO_REUSE):\n",
    "\n",
    "            # get user embedding p and item embedding q\n",
    "            self.mlp_p = tf.reduce_sum(\n",
    "                tf.nn.embedding_lookup(self.embedding_mlp_P, self.user_input), 1\n",
    "            )\n",
    "            self.mlp_q = tf.reduce_sum(\n",
    "                tf.nn.embedding_lookup(self.embedding_mlp_Q, self.item_input), 1\n",
    "            )\n",
    "\n",
    "            # concatenate user and item vector\n",
    "            output = tf.concat([self.mlp_p, self.mlp_q], 1)\n",
    "\n",
    "            # MLP Layers\n",
    "            for layer_size in self.layer_sizes[1:]:\n",
    "                output = v1.layers.dense(\n",
    "                    output, layer_size, activation=tf.nn.relu\n",
    "                )\n",
    "            self.mlp_vector = output\n",
    "\n",
    "            # self.output = tf.sigmoid(tf.reduce_sum(self.mlp_vector, axis=1, keepdims=True))\n",
    "\n",
    "        with tf.compat.v1.variable_scope(\"ncf\", reuse=tf.compat.v1.AUTO_REUSE):\n",
    "\n",
    "            if self.model_type == \"gmf\":\n",
    "                # GMF only\n",
    "                output = v1.layers.dense(\n",
    "                    self.gmf_vector,\n",
    "                    1,\n",
    "                    activation=None,\n",
    "                    biases_initializer=None,\n",
    "                )\n",
    "                self.output = tf.sigmoid(output)\n",
    "\n",
    "            elif self.model_type == \"mlp\":\n",
    "                # MLP only\n",
    "                output = v1.layers.dense(\n",
    "                    self.mlp_vector,\n",
    "                    1,\n",
    "                    activation=None,\n",
    "                    bias_initializer=None,\n",
    "                )\n",
    "                self.output = tf.sigmoid(output)\n",
    "\n",
    "            elif self.model_type == \"neumf\":\n",
    "                # concatenate GMF and MLP vector\n",
    "                self.ncf_vector = tf.concat([self.gmf_vector, self.mlp_vector], 1)\n",
    "                # get predicted rating score\n",
    "                output = v1.layers.dense(\n",
    "                    self.ncf_vector,\n",
    "                    1,\n",
    "                    activation=None,\n",
    "                    bias_initializer=None,\n",
    "                )\n",
    "                self.output = tf.sigmoid(output)\n",
    "\n",
    "        with v1.variable_scope(\"loss\", reuse=v1.AUTO_REUSE):\n",
    "\n",
    "            # set loss function\n",
    "            self.loss = v1.losses.log_loss(self.labels, self.output)\n",
    "\n",
    "        with v1.variable_scope(\"optimizer\", reuse=v1.AUTO_REUSE):\n",
    "\n",
    "            # set optimizer\n",
    "            self.optimizer = v1.train.AdamOptimizer(\n",
    "                learning_rate=self.learning_rate\n",
    "            ).minimize(self.loss)\n",
    "\n",
    "    def save(self, dir_name):\n",
    "        \"\"\" save model parameters in `dir_name`\n",
    "            Args:\n",
    "                dir_name (str) : directory name, which should be folder name instead of file name\n",
    "                    we will create a new directory if not existing.\n",
    "        \"\"\"\n",
    "        # save trained model\n",
    "        if not os.path.exists(dir_name):\n",
    "            os.makedirs(dir_name)\n",
    "        saver = tf.compat.v1.train.Saver()\n",
    "        saver.save(self.sess, os.path.join(dir_name, \"model.ckpt\"))\n",
    "\n",
    "    def load(self, gmf_dir=None, mlp_dir=None, neumf_dir=None, alpha=0.5):\n",
    "        \"\"\" load model parameters for further use.\n",
    "            GMF model --> load parameters in `gmf_dir`\n",
    "            MLP model --> load parameters in `mlp_dir`\n",
    "            NeuMF model --> load parameters in `neumf_dir` or in `gmf_dir` and `mlp_dir`\n",
    "            Args:\n",
    "                gmf_dir, mlp_dir, neumf_dir ( str or None ): model parameters directory name\n",
    "            Returns:\n",
    "                load parameters in this model\n",
    "        \"\"\"\n",
    "\n",
    "        # load pre-trained model\n",
    "        if self.model_type == \"gmf\" and gmf_dir is not None:\n",
    "            saver = v1.train.Saver()\n",
    "            saver.restore(self.sess, os.path.join(gmf_dir, \"model.ckpt\"))\n",
    "\n",
    "        elif self.model_type == \"mlp\" and mlp_dir is not None:\n",
    "            saver = v1.train.Saver()\n",
    "            saver.restore(self.sess, os.path.join(mlp_dir, \"model.ckpt\"))\n",
    "\n",
    "        elif self.model_type == \"neumf\" and neumf_dir is not None:\n",
    "            saver = v1.train.Saver()\n",
    "            saver.restore(self.sess, os.path.join(neumf_dir, \"model.ckpt\"))\n",
    "\n",
    "        elif self.model_type == \"neumf\" and gmf_dir is not None and mlp_dir is not None:\n",
    "            # load neumf using gmf and mlp\n",
    "            self._load_neumf(gmf_dir, mlp_dir, alpha)\n",
    "\n",
    "        else:\n",
    "            raise NotImplementedError\n",
    "\n",
    "    def _load_neumf(self, gmf_dir, mlp_dir, alpha):\n",
    "        \"\"\" load gmf and mlp model parameters for further use in NeuMF.\n",
    "            NeuMF model --> load parameters in `gmf_dir` and `mlp_dir`\n",
    "            Args:\n",
    "                gmf_dir, mlp_dir ( str or None ): model parameters directory name\n",
    "                alpha ( float ): the concatenation hyper-parameter for gmf and mlp output layer\n",
    "            Returns:\n",
    "                load parameters in NeuMF model\n",
    "        \"\"\"\n",
    "        # load gmf part\n",
    "        variables = v1.global_variables()\n",
    "        # get variables with 'gmf'\n",
    "        var_flow_restore = [\n",
    "            val for val in variables if \"gmf\" in val.name and \"ncf\" not in val.name\n",
    "        ]\n",
    "        # load 'gmf' variable\n",
    "        saver = v1.train.Saver(var_flow_restore)\n",
    "        # restore\n",
    "        saver.restore(self.sess, os.path.join(gmf_dir, \"model.ckpt\"))\n",
    "\n",
    "        # load mlp part\n",
    "        variables = v1.global_variables()\n",
    "        # get variables with 'gmf'\n",
    "        var_flow_restore = [\n",
    "            val for val in variables if \"mlp\" in val.name and \"ncf\" not in val.name\n",
    "        ]\n",
    "        # load 'gmf' variable\n",
    "        saver = v1.train.Saver(var_flow_restore)\n",
    "        # restore\n",
    "        saver.restore(self.sess, os.path.join(mlp_dir, \"model.ckpt\"))\n",
    "\n",
    "        # concat pretrain h_from_gmf and h_from_mlp\n",
    "        vars_list = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, scope=\"ncf\")\n",
    "\n",
    "        assert len(vars_list) == 1\n",
    "        ncf_fc = vars_list[0]\n",
    "\n",
    "        # get weight from gmf and mlp\n",
    "        gmf_fc = tf.contrib.framework.load_variable(gmf_dir, ncf_fc.name)\n",
    "        mlp_fc = tf.contrib.framework.load_variable(mlp_dir, ncf_fc.name)\n",
    "\n",
    "        # load fc layer by tf.concat\n",
    "        assign_op = tf.assign(\n",
    "            ncf_fc, tf.concat([alpha * gmf_fc, (1 - alpha) * mlp_fc], axis=0)\n",
    "        )\n",
    "        self.sess.run(assign_op)\n",
    "\n",
    "    def fit(self, data):\n",
    "        \"\"\" fit model with training data\n",
    "            \n",
    "            Args: \n",
    "                data ( NCFDataset ): initilized Dataset in ./dataset.py\n",
    "        \"\"\"\n",
    "\n",
    "        # get user and item mapping dict\n",
    "        self.user2id = data.user2id\n",
    "        self.item2id = data.item2id\n",
    "        self.id2user = data.id2user\n",
    "        self.id2item = data.id2item\n",
    "\n",
    "        # loop for n_epochs\n",
    "        for epoch_count in range(1, self.n_epochs + 1):\n",
    "\n",
    "            # negative sampling for training\n",
    "            train_begin = time()\n",
    "            data.negative_sampling()\n",
    "\n",
    "            # initialize\n",
    "            train_loss = []\n",
    "\n",
    "            # calculate loss and update NCF parameters\n",
    "            for user_input, item_input, labels in data.train_loader(self.batch_size):\n",
    "\n",
    "                user_input = np.array([self.user2id[x] for x in user_input])\n",
    "                item_input = np.array([self.item2id[x] for x in item_input])\n",
    "                labels = np.array(labels)\n",
    "\n",
    "                feed_dict = {\n",
    "                    self.user_input: user_input[..., None],\n",
    "                    self.item_input: item_input[..., None],\n",
    "                    self.labels: labels[..., None],\n",
    "                }\n",
    "\n",
    "                # get loss and execute optimization\n",
    "                loss, _ = self.sess.run([self.loss, self.optimizer], feed_dict)\n",
    "                train_loss.append(loss)\n",
    "            train_time = time() - train_begin\n",
    "\n",
    "            # output every self.verbose\n",
    "            if self.verbose and epoch_count % self.verbose == 0:\n",
    "                logger.info(\n",
    "                    \"Epoch %d [%.2fs]: train_loss = %.6f \"\n",
    "                    % (epoch_count, train_time, sum(train_loss) / len(train_loss))\n",
    "                )\n",
    "\n",
    "    def predict(self, user_input, item_input, is_list=False):\n",
    "        \"\"\" predict function of this trained model\n",
    "            Args:\n",
    "                user_input ( list or element of list ): userID or userID list \n",
    "                item_input ( list or element of list ): itemID or itemID list\n",
    "                is_list ( bool ): if true, the input is list type\n",
    "                noting that list-wise type prediction is faster than element-wise's.\n",
    "            Returns:\n",
    "                list or float: list of predicted rating or predicted rating score. \n",
    "        \"\"\"\n",
    "\n",
    "        if is_list:\n",
    "            output = self._predict(user_input, item_input)\n",
    "            return list(output.reshape(-1))\n",
    "\n",
    "        else:\n",
    "            output = self._predict(np.array([user_input]), np.array([item_input]))\n",
    "            return float(output.reshape(-1)[0])\n",
    "\n",
    "    def _predict(self, user_input, item_input):\n",
    "\n",
    "        # index converting\n",
    "        user_input = np.array([self.user2id[x] for x in user_input])\n",
    "        item_input = np.array([self.item2id[x] for x in item_input])\n",
    "\n",
    "        # get feed dict\n",
    "        feed_dict = {\n",
    "            self.user_input: user_input[..., None],\n",
    "            self.item_input: item_input[..., None],\n",
    "        }\n",
    "\n",
    "        # calculate predicted score\n",
    "        output = self.sess.run(self.output, feed_dict)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 424
    },
    "executionInfo": {
     "elapsed": 14450,
     "status": "ok",
     "timestamp": 1668978547259,
     "user": {
      "displayName": "Marcela Medicina",
      "userId": "17288344570535601151"
     },
     "user_tz": 180
    },
    "id": "l_Cv6r5m2UQL",
    "outputId": "112632ab-c437-4724-b4be-7dff382ad61f"
   },
   "outputs": [],
   "source": [
    "train = pd.read_csv('./train_split.csv')\n",
    "test = pd.read_csv('./test_split.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 424
    },
    "executionInfo": {
     "elapsed": 14450,
     "status": "ok",
     "timestamp": 1668978547259,
     "user": {
      "displayName": "Marcela Medicina",
      "userId": "17288344570535601151"
     },
     "user_tz": 180
    },
    "id": "l_Cv6r5m2UQL",
    "outputId": "112632ab-c437-4724-b4be-7dff382ad61f"
   },
   "outputs": [],
   "source": [
    "# Caso os ratings não estejam normalizados, rodar essa célula.\n",
    "train['rating'] = np.where(train['rating']>2.5, 1, 0)\n",
    "test['rating'] = np.where(test['rating']>2.5, 1, 0)\n",
    "train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GMAFy-bVC1_I"
   },
   "outputs": [],
   "source": [
    "# Model parameters\n",
    "EPOCHS = 100\n",
    "BATCH_SIZE = 256\n",
    "TOP_K = 10\n",
    "\n",
    "# Set None for non-deterministic results\n",
    "seed=710 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GMAFy-bVC1_I"
   },
   "outputs": [],
   "source": [
    "# note, você deve utilizar col_user, col_item de acordo com as colunas do seu dataset\n",
    "data = NCFDataset(train=train, test=test, seed=seed, col_user ='userId', col_item = 'movieId')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 424
    },
    "executionInfo": {
     "elapsed": 371,
     "status": "ok",
     "timestamp": 1669045844796,
     "user": {
      "displayName": "Marcela Medicina",
      "userId": "18221795483743632611"
     },
     "user_tz": 180
    },
    "id": "jZCz_zCpDMJ3",
    "outputId": "e0dc9abd-2787-4e08-f2fd-e854a11d7ee9"
   },
   "outputs": [],
   "source": [
    "train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rooWDpxU_NGX",
    "outputId": "697e0a1a-4634-4d58-fd3d-aba5f31ebf63"
   },
   "outputs": [],
   "source": [
    "model = NCF(n_users=data.n_users,n_items=data.n_items,model_type=\"NeuMF\",n_factors=4,layer_sizes=[16,8,4],\\\n",
    "            n_epochs=EPOCHS,batch_size=BATCH_SIZE,learning_rate=1e-3,verbose=10,seed=seed)\n",
    "# n_factors (int): Dimension of latent space.\n",
    "# layer_sizes (list): Number of layers for MLP.\n",
    "\n",
    "# training the model\n",
    "model.fit(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yR-DeihHs8pZ"
   },
   "outputs": [],
   "source": [
    "predictions = [[row.userId, row.movieId, model.predict(row.userId, row.movieId)]\n",
    "               for (_, row) in test.iterrows()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "executionInfo": {
     "elapsed": 17,
     "status": "ok",
     "timestamp": 1669046910362,
     "user": {
      "displayName": "Marcela Medicina",
      "userId": "18221795483743632611"
     },
     "user_tz": 180
    },
    "id": "HhNWw9VVs6T3",
    "outputId": "6a46a715-5c06-44a9-e845-336c3ebe4521"
   },
   "outputs": [],
   "source": [
    "# saving the predictions in a dataframe\n",
    "predictions = pd.DataFrame(predictions, columns=['userId', 'movieId', 'prediction'])\n",
    "predictions.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 289
    },
    "executionInfo": {
     "elapsed": 10,
     "status": "error",
     "timestamp": 1669047370067,
     "user": {
      "displayName": "Marcela Medicina",
      "userId": "18221795483743632611"
     },
     "user_tz": 180
    },
    "id": "gyEcEVWW83fL",
    "outputId": "fc9a27c3-1e24-4ebc-bd4b-a841a857a29b"
   },
   "outputs": [],
   "source": [
    "#salvar o modelo\n",
    "model.save('/content/drive/MyDrive/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tZu_Cu1eBvl9"
   },
   "outputs": [],
   "source": [
    "users, items, preds = [], [], []\n",
    "item = list(train.movieId.unique())\n",
    "for user in train.userId.unique():\n",
    "    user = [user] * len(item) \n",
    "    users.extend(user)\n",
    "    items.extend(item)\n",
    "    preds.extend(list(model.predict(user, item, is_list=True)))\n",
    "\n",
    "all_predictions = pd.DataFrame(data={\"userId\": users, \"movieId\":items, \"prediction\":preds})\n",
    "\n",
    "merged = pd.merge(train, all_predictions, on=[\"userId\", \"movieId\"], how=\"outer\")\n",
    "all_predictions = merged[merged.rating.isnull()].drop('rating', axis=1)\n",
    "\n",
    "eval_map = map_at_k(test, all_predictions, col_prediction='prediction', k=TOP_K, col_user ='userId', col_item = 'movieId')\n",
    "eval_ndcg = ndcg_at_k(test, all_predictions, col_prediction='prediction', k=TOP_K, col_user ='userId', col_item = 'movieId')\n",
    "eval_precision = precision_at_k(test, all_predictions, col_prediction='prediction', k=TOP_K,col_user ='userId', col_item = 'movieId')\n",
    "eval_recall = recall_at_k(test, all_predictions, col_prediction='prediction', k=TOP_K,col_user ='userId', col_item = 'movieId')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Agora, é só salvar as precisões na pasta de resultados :D "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
